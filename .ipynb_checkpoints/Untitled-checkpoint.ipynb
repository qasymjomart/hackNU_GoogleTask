{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0317 04:01:32.418262 139929440462656 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import caption\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "from encoder import Encoder\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yerlan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 04:01:50.487209 139929440462656 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "myenc = Encoder()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_to_sentence_matrix_clear(directory):\n",
    "    sentence_dict = {}\n",
    "    for j in range(1,7):\n",
    "        filename = directory + \"/\" + str(j) + \".jpg\"\n",
    "        checkpoint = torch.load('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar')\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder = decoder.to(device)\n",
    "        decoder.eval()\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "\n",
    "        # Load word map (word2ix)\n",
    "        with open('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json' , 'r') as t:\n",
    "            word_map = json.load(t)\n",
    "        rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "\n",
    "        sentence_array = []\n",
    "        # Encode, decode with 0attention and beam search\n",
    "        for i in range(1, 6):\n",
    "            seq, alphas = caption.caption_image_beam_search(encoder, decoder, filename, word_map, i)\n",
    "            alphas = torch.FloatTensor(alphas)\n",
    "\n",
    "            # Visualize caption and attention of best sequence\n",
    "            sentence_array.append(clear(caption.return_sentence(filename, seq, alphas, rev_word_map)))\n",
    "        sentence_dict[j] = sentence_array\n",
    "\n",
    "    return sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_to_sentence_matrix(directory):\n",
    "    sentence_dict = {}\n",
    "    for j in range(1,7):\n",
    "        filename = directory + \"/\" + str(j) + \".jpg\"\n",
    "        checkpoint = torch.load('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar')\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder = decoder.to(device)\n",
    "        decoder.eval()\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "\n",
    "        # Load word map (word2ix)\n",
    "        with open('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json' , 'r') as t:\n",
    "            word_map = json.load(t)\n",
    "        rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "\n",
    "        sentence_array = []\n",
    "        # Encode, decode with 0attention and beam search\n",
    "        for i in range(1, 6):\n",
    "            seq, alphas = caption.caption_image_beam_search(encoder, decoder, filename, word_map, i)\n",
    "            alphas = torch.FloatTensor(alphas)\n",
    "\n",
    "            # Visualize caption and attention of best sequence\n",
    "            sentence_array.append(caption.return_sentence(filename, seq, alphas, rev_word_map))\n",
    "        sentence_dict[j] = sentence_array\n",
    "\n",
    "    return sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_matrix(generated_matrix_of_sentences, given_sentence):\n",
    "    #For generating matrix of cosine similarities\n",
    "    #myenc = Encoder()\n",
    "    emgiven = myenc.encode(given_sentence)\n",
    "    res = []\n",
    "    for i in range(len(generated_matrix_of_sentences)):\n",
    "        emgenerated = myenc.encode(generated_matrix_of_sentences[i])\n",
    "        a = myenc.cosine_similarity(emgiven, emgenerated)[0][0]\n",
    "        res.append(a)\n",
    "        \n",
    "        #For getting the average of the matrix\n",
    "        res_np = np.array(res)\n",
    "        mean = np.mean(res_np)\n",
    "        minimum = np.min(res_np)\n",
    "        \n",
    "    return mean, minimum\n",
    "\n",
    "\n",
    "def filter_input1(directory):\n",
    "    with open(directory) as file:\n",
    "        text = file.read()\n",
    "        sentence = text.translate({ord(i):None for i in '.!@#$?]}{[,\\n'}) #remove all unnecessary elements\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def filter_input2(path, stem=False):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    f = open(\"./stop.txt\", 'r')\n",
    "    stop = f.read().split('\\n')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopWords |= set(stop)\n",
    "\n",
    "    #path = \"./test-updated/\" + str(dir_num) + \"/input\"\n",
    "    with open(path) as file:\n",
    "        text = file.read()\n",
    "        sentence = text.translate({ord(i):None for i in '.!@#$?]}{[,'}) #remove all unnecessary elements\n",
    "        \n",
    "    def stem_filter(sentence):\n",
    "        return \" \".join(wordnet_lemmatizer.lemmatize(word.lower(), pos='v') for word in word_tokenize(sentence) if word.lower() not in stopWords)\n",
    "\n",
    "    def tagged_filter(sentence):\n",
    "        tagged_sent = nltk.pos_tag(word_tokenize(sentence))\n",
    "        selected = ['CD', 'FW', 'JJ', 'NN', 'NNP', 'NNS', 'NNPS', 'VBG']\n",
    "        return \" \".join([word[0] for word in tagged_sent if word[1] in selected])\n",
    "    \n",
    "    return stem_filter(sentence) if stem else tagged_filter(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear(raw, stem=False):\n",
    "    \n",
    "    f = open(\"./stop.txt\", 'r')\n",
    "    stop = f.read().split('\\n')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopWords |= set(stop)\n",
    "    \n",
    "    def stem_filter(sentence):\n",
    "            return \" \".join(wordnet_lemmatizer.lemmatize(word.lower(), pos='v') for word in word_tokenize(sentence) if word.lower() not in stopWords)\n",
    "\n",
    "    def tagged_filter(sentence):\n",
    "            tagged_sent = nltk.pos_tag(word_tokenize(sentence))\n",
    "            selected = ['CD', 'FW', 'JJ', 'NN', 'NNP', 'NNS', 'NNPS', 'VBG']\n",
    "            return \" \".join([word[0] for word in tagged_sent if word[1] in selected])\n",
    "    \n",
    "    return stem_filter(raw) if stem else tagged_filter(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a colorful bird is perched on a rock'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_dictionary[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colorful bird rock'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear(sentence_dictionary[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 25\n",
    "path = \"/home/yerlan/HackNU/images/\"\n",
    "sentence_matrix = []\n",
    "#loop iterating over directories\n",
    "#for i in range(1,n):\n",
    "directory = path + str(i)\n",
    "    # below is for iterating over images in the folder, please uncomment to activate\n",
    "sentence_dictionary_clear = directory_to_sentence_matrix_clear(directory)\n",
    "sentence_dictionary = directory_to_sentence_matrix(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['a colorful bird is perched on a rock',\n",
       "  'a colorful bird is perched on a rock',\n",
       "  'a colorful bird sitting on top of a white bird',\n",
       "  'a bird that is sitting on top of a tree',\n",
       "  'a bird sitting on top of a white bird'],\n",
       " 2: ['a yellow and black bird sitting on a branch',\n",
       "  'a yellow and black bird sitting on a branch',\n",
       "  'a yellow and black bird sitting on a branch',\n",
       "  'a yellow and black bird sitting on a branch',\n",
       "  'a yellow and black bird sitting on a branch'],\n",
       " 3: ['a colorful bird is perched on a branch',\n",
       "  'a colorful bird is perched on a branch',\n",
       "  'a colorful bird is perched on a branch',\n",
       "  'a colorful bird is perched on a branch',\n",
       "  'a colorful bird is perched on a branch'],\n",
       " 4: ['a bird is flying in the sky at sunset',\n",
       "  'a bird is flying in the sky at sunset',\n",
       "  'a bird that is flying in the sky',\n",
       "  'a bird that is flying in the sky',\n",
       "  'a bird that is flying in the sky'],\n",
       " 5: ['a red parrot with a orange beak on its head',\n",
       "  'a close up of an orange and a red bird',\n",
       "  'a close up of a bird with a orange beak',\n",
       "  'a close up of a bird with a orange beak',\n",
       "  'a close up of an orange and a bird'],\n",
       " 6: ['a blue and yellow bird sitting on a branch',\n",
       "  'a blue and yellow bird sitting on a branch',\n",
       "  'a blue and yellow bird sitting on a branch',\n",
       "  'a bird is perched on a tree branch',\n",
       "  'a bird is perched on a tree branch']}"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['colorful bird rock',\n",
       "  'colorful bird rock',\n",
       "  'colorful bird sitting top white bird',\n",
       "  'bird sitting top tree',\n",
       "  'bird sitting top white bird'],\n",
       " 2: ['yellow black bird sitting branch',\n",
       "  'yellow black bird sitting branch',\n",
       "  'yellow black bird sitting branch',\n",
       "  'yellow black bird sitting branch',\n",
       "  'yellow black bird sitting branch'],\n",
       " 3: ['colorful bird branch',\n",
       "  'colorful bird branch',\n",
       "  'colorful bird branch',\n",
       "  'colorful bird branch',\n",
       "  'colorful bird branch'],\n",
       " 4: ['bird flying sky sunset',\n",
       "  'bird flying sky sunset',\n",
       "  'bird flying sky',\n",
       "  'bird flying sky',\n",
       "  'bird flying sky'],\n",
       " 5: ['red parrot orange beak head',\n",
       "  'close orange red bird',\n",
       "  'close bird orange beak',\n",
       "  'close bird orange beak',\n",
       "  'close orange bird'],\n",
       " 6: ['blue yellow bird sitting branch',\n",
       "  'blue yellow bird sitting branch',\n",
       "  'blue yellow bird sitting branch',\n",
       "  'bird tree branch',\n",
       "  'bird tree branch']}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_dictionary_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dir = directory + \"/input\"\n",
    "yer_clear = sentence_dictionary_clear\n",
    "yer = sentence_dictionary\n",
    "given = filter_input1(sent_dir)\n",
    "zhanik = filter_input2(sent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yerlan/HackNU/images/25/input'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the sad yellow bird'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sad yellow bird'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhanik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Zhanibek\n",
      "For Image #1\n",
      "\tmean: 0.4096243608365836\n",
      "\tminimum: 0.3965536963243662\n",
      "For Image #2\n",
      "\tmean: 0.3489633291440929\n",
      "\tminimum: 0.3489633291440929\n",
      "For Image #3\n",
      "\tmean: 0.40977945980221814\n",
      "\tminimum: 0.4097794598022182\n",
      "For Image #4\n",
      "\tmean: 0.42268635943427535\n",
      "\tminimum: 0.41688467025216036\n",
      "For Image #5\n",
      "\tmean: 0.3749326108796355\n",
      "\tminimum: 0.3490396970231152\n",
      "For Image #6\n",
      "\tmean: 0.404150758209006\n",
      "\tminimum: 0.348502541326861\n"
     ]
    }
   ],
   "source": [
    "print(\"Without Zhanibek\")\n",
    "\n",
    "for k in range(len(yer)):\n",
    "    cos = cos_matrix(yer_clear[k+1], given)\n",
    "    print(\"For Image #\" + str(k+1))\n",
    "    print(\"\\tmean: \" + str(cos[0]))\n",
    "    print(\"\\tminimum: \" + str(cos[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Zhanibek\n",
      "For Image #1\n",
      "\tmean: 0.40871823542167374\n",
      "\tminimum: 0.377202865471686\n",
      "For Image #2\n",
      "\tmean: 0.3604732979238896\n",
      "\tminimum: 0.3604732979238896\n",
      "For Image #3\n",
      "\tmean: 0.4038188169434541\n",
      "\tminimum: 0.4038188169434541\n",
      "For Image #4\n",
      "\tmean: 0.4243829586466733\n",
      "\tminimum: 0.41909445697381176\n",
      "For Image #5\n",
      "\tmean: 0.37836988422412965\n",
      "\tminimum: 0.35226813748022756\n",
      "For Image #6\n",
      "\tmean: 0.40608096753017814\n",
      "\tminimum: 0.35776250586739256\n"
     ]
    }
   ],
   "source": [
    "print(\"With Zhanibek\")\n",
    "\n",
    "for j in range(len(yer)):\n",
    "    cos = cos_matrix(yer_clear[j+1], zhanik)\n",
    "    print(\"For Image #\" + str(j+1))\n",
    "    print(\"\\tmean: \" + str(cos[0]))\n",
    "    print(\"\\tminimum: \" + str(cos[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Zhanibek\n",
      "For Image #1\n",
      "\tmean: 0.5066130429301762\n",
      "\tminimum: 0.4768590122992006\n",
      "For Image #2\n",
      "\tmean: 0.45016514464913504\n",
      "\tminimum: 0.45016514464913504\n",
      "For Image #3\n",
      "\tmean: 0.48687772990844297\n",
      "\tminimum: 0.486877729908443\n",
      "For Image #4\n",
      "\tmean: 0.5463384893758059\n",
      "\tminimum: 0.5253235032519536\n",
      "For Image #5\n",
      "\tmean: 0.411536028033466\n",
      "\tminimum: 0.40601090747239277\n",
      "For Image #6\n",
      "\tmean: 0.4901792384418341\n",
      "\tminimum: 0.46104556477887226\n"
     ]
    }
   ],
   "source": [
    "print(\"Without Zhanibek\")\n",
    "\n",
    "for k in range(len(yer)):\n",
    "    cos = cos_matrix(yer[k+1], given)\n",
    "    print(\"For Image #\" + str(k+1))\n",
    "    print(\"\\tmean: \" + str(cos[0]))\n",
    "    print(\"\\tminimum: \" + str(cos[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Zhanibek\n",
      "For Image #1\n",
      "\tmean: 0.5159507174524425\n",
      "\tminimum: 0.5042032066075544\n",
      "For Image #2\n",
      "\tmean: 0.4816081932071228\n",
      "\tminimum: 0.4816081932071228\n",
      "For Image #3\n",
      "\tmean: 0.4784824230532404\n",
      "\tminimum: 0.4784824230532404\n",
      "For Image #4\n",
      "\tmean: 0.5613724743715549\n",
      "\tminimum: 0.5455596794224105\n",
      "For Image #5\n",
      "\tmean: 0.43768070562912087\n",
      "\tminimum: 0.4304505154594901\n",
      "For Image #6\n",
      "\tmean: 0.49428511099947875\n",
      "\tminimum: 0.478866273823511\n"
     ]
    }
   ],
   "source": [
    "print(\"With Zhanibek\")\n",
    "\n",
    "for j in range(len(yer)):\n",
    "    cos = cos_matrix(yer[j+1], zhanik)\n",
    "    print(\"For Image #\" + str(j+1))\n",
    "    print(\"\\tmean: \" + str(cos[0]))\n",
    "    print(\"\\tminimum: \" + str(cos[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
