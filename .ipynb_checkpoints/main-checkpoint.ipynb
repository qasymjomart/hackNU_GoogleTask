{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import caption\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "from encoder import Encoder\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0317 09:53:15.173402 139715706857280 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "myenc = Encoder()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear(raw, stem=False):\n",
    "    \n",
    "    f = open(\"./stop.txt\", 'r')\n",
    "    stop = f.read().split('\\n')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    stopWords |= set(stop)\n",
    "    \n",
    "    def stem_filter(sentence):\n",
    "            return \" \".join(wordnet_lemmatizer.lemmatize(word.lower(), pos='v') for word in word_tokenize(sentence) if word.lower() not in stopWords)\n",
    "\n",
    "    def tagged_filter(sentence):\n",
    "            tagged_sent = nltk.pos_tag(word_tokenize(sentence))\n",
    "            selected = ['CD', 'FW', 'JJ', 'NN', 'NNP', 'NNS', 'NNPS', 'VBG']\n",
    "            return \" \".join([word[0] for word in tagged_sent if word[1] in selected])\n",
    "    \n",
    "    return stem_filter(raw) if stem else tagged_filter(raw)\n",
    "\n",
    "def directory_to_sentence_matrix_clear(directory):\n",
    "    sentence_dict = {}\n",
    "    for j in range(1,7):\n",
    "        filename = directory + \"/\" + str(j) + \".jpg\"\n",
    "        checkpoint = torch.load('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar')\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder = decoder.to(device)\n",
    "        decoder.eval()\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "\n",
    "        # Load word map (word2ix)\n",
    "        with open('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json' , 'r') as t:\n",
    "            word_map = json.load(t)\n",
    "        rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "\n",
    "        sentence_array = []\n",
    "        # Encode, decode with attention and beam search\n",
    "        for i in range(1, 6):\n",
    "            seq, alphas = caption.caption_image_beam_search(encoder, decoder, filename, word_map, i)\n",
    "            alphas = torch.FloatTensor(alphas)\n",
    "\n",
    "            # Visualize caption and attention of best sequence\n",
    "            sentence_array.append(clear(caption.return_sentence(filename, seq, alphas, rev_word_map)))\n",
    "        sentence_dict[j] = sentence_array\n",
    "\n",
    "    return sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_to_sentence_matrix(directory):\n",
    "    sentence_dict = {}\n",
    "    for j in range(1,7):\n",
    "        filename = directory + \"/\" + str(j) + \".jpg\"\n",
    "        checkpoint = torch.load('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar')\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder = decoder.to(device)\n",
    "        decoder.eval()\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder = encoder.to(device)\n",
    "        encoder.eval()\n",
    "\n",
    "        # Load word map (word2ix)\n",
    "        with open('/home/yerlan/HackNU/a-PyTorch-Tutorial-to-Image-Captioning/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json' , 'r') as t:\n",
    "            word_map = json.load(t)\n",
    "        rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "\n",
    "        sentence_array = []\n",
    "        # Encode, decode with 0attention and beam search\n",
    "        for i in range(1, 6):\n",
    "            seq, alphas = caption.caption_image_beam_search(encoder, decoder, filename, word_map, i)\n",
    "            alphas = torch.FloatTensor(alphas)\n",
    "\n",
    "            # Visualize caption and attention of best sequence\n",
    "            sentence_array.append(caption.return_sentence(filename, seq, alphas, rev_word_map))\n",
    "        sentence_dict[j] = sentence_array\n",
    "\n",
    "    return sentence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_matrix(generated_matrix_of_sentences, given_sentence):\n",
    "    #For generating matrix of cosine similarities\n",
    "    #myenc = Encoder()\n",
    "    emgiven = myenc.encode(given_sentence)\n",
    "    res = []\n",
    "    for i in range(len(generated_matrix_of_sentences)):\n",
    "        emgenerated = myenc.encode(generated_matrix_of_sentences[i])\n",
    "        a = myenc.cosine_similarity(emgiven, emgenerated)[0][0]\n",
    "        res.append(a)\n",
    "        \n",
    "        #For getting the average of the matrix\n",
    "        res_np = np.array(res)\n",
    "        mean = np.mean(res_np)\n",
    "        minimum = np.min(res_np)\n",
    "        \n",
    "    return mean, minimum\n",
    "\n",
    "\n",
    "def read_file(directory):\n",
    "    with open(directory) as file:\n",
    "        text = file.read()\n",
    "        sentence = text.translate({ord(i):None for i in '.!@#$?]}{[,\\n'}) #remove all unnecessary elements\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 52.0\n",
      "0.1 60.0\n",
      "0.2 64.0\n",
      "0.3 64.0\n",
      "0.4 64.0\n",
      "0.5 64.0\n",
      "0.6 64.0\n",
      "0.7 64.0\n",
      "0.8 64.0\n",
      "0.9 64.0\n",
      "CPU times: user 50min 35s, sys: 6min 1s, total: 56min 36s\n",
      "Wall time: -1.93e+13 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for alpha in range(0, 10):\n",
    "    alpha /= 10.0\n",
    "    count = 0\n",
    "    for i in range(1, 26):\n",
    "        path = \"/home/yerlan/HackNU/images/\"\n",
    "        sentence_matrix = []\n",
    "        #loop iterating over directories\n",
    "        #for i in range(1,n):\n",
    "        directory = path + str(i)\n",
    "            # below is for iterating over images in the folder, please uncomment to activate\n",
    "        sentence_dictionary_clear = directory_to_sentence_matrix_clear(directory)\n",
    "    #     sentence_dictionary = directory_to_sentence_matrix(directory)\n",
    "        \n",
    "        sent_dir = directory + \"/input\"\n",
    "        orig_sent = read_file(sent_dir)\n",
    "        res = []\n",
    "        for k in range(len(sentence_dictionary_clear)):\n",
    "            cos = cos_matrix(sentence_dictionary_clear[k+1], orig_sent)\n",
    "            means.append(cos[0])\n",
    "            minimums.append(cos[1])\n",
    "            res.append((str(k+1), alpha*cos[0]+(1 - alpha)*cos[1]))\n",
    "        label = sorted(res, key=lambda p: p[1])[0][0]\n",
    "        if(label == list[i]):\n",
    "            count += 1\n",
    "        output = directory + \"/output\"\n",
    "        f = open(output, 'w')\n",
    "        f.write(label)\n",
    "        f.close()\n",
    "    print(str(alpha) + \" \" +  str((float(count) / 25.0) * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [6,6,3,4,1,3,3,3,3,1,2,4,3,3,1,6,3,5,2,3,1,3,2,2,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
